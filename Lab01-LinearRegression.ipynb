{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dw29NSYmFpyS"
   },
   "source": [
    "# Lab01: Linear Regression.\n",
    "\n",
    "- Student ID: 19127358\n",
    "- Student name: Nguyễn Trọng Đạt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHR1Zj5GFpyT"
   },
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Rename your notebook with `ID.ipynb` (for example, if your ID is 1234567, rename your notebook with `1234567.ipynb`) and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REHkv-y8FpyU"
   },
   "source": [
    "### 1. The hypothesis set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e75OMY0KFpyU"
   },
   "source": [
    "- Linear regression is a **linear** model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n",
    "- Generally, a linear model will make predictions by calculating a weighted sum of the input features (independent variables). \n",
    "$$ \\hat{y}=w_0+w_1x_1+w_2x_2+...+w_nx_n $$\n",
    "    - $\\hat{y}$ is the predicted value.\n",
    "    - $n$ is the number of features.\n",
    "    - $x_i$ is the $i^{th}$ feature value.\n",
    "    - $w_j$ is the $j^{th}$ model parameter (including the bias term $w_0$ and the feature weights $w_1,w_2,...w_n)$.\n",
    "$$\\hat{y}=h_{\\mathbf{w}}\\left(\\mathbf{x}\\right)=\\mathbf{w}^{T}\\cdot\\mathbf{x}$$\n",
    "    - $\\mathbf{w}$ is the model **parameter vector** (including the bias term $w_0$ and the feature weights $w_1,w_2,...w_n$).\n",
    "    - $\\mathbf{w}^T$ is a transpose  of $\\mathbf{w}$ (a row vector insteade of column vector).\n",
    "    - $\\mathbf{x}$ is the instance's **feature vector**, *containing* $x_0$ to $x_n$, with $x_0$ *always equal to* 1.\n",
    "    - $\\mathbf{w}^{T}\\cdot\\mathbf{x}$ is the dot product of $\\mathbf{w}^T$ and $\\mathbf{x}$.\n",
    "    - $h_{\\mathbf{w}}$ is the hypothesis function, using the parameters $\\mathbf{w}$.\n",
    "![Bias](Bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5l8F4lnjFpyV"
   },
   "source": [
    "### 2. Performance measure and the learning goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdJNZ2q6FpyX"
   },
   "source": [
    "- Before we start to train the model, we need to determine how good the model fits the training data. There are a couple of ways to determine the level of quality, but we are going to use the most popular one and that is the **MSE** (Mean Square Error). We need to find the value for $\\mathbf{w}$ that will minimize the MSE:\n",
    "$$\\mathbf{w}=\\arg\\min MSE_{\\mathcal{D}_{train}}$$\n",
    "\n",
    "\n",
    "- MSE on the train set $\\mathcal{D}_{train}$ denoted as $\\left(\\mathbf{X},\\mathbf{y}\\right)$ including m samples $\\left\\{\\left(\\mathbf{x}_1,y_1\\right),\\left(\\mathbf{x}_2,y_2\\right),...\\left(\\mathbf{x}_m,y_m\\right)\\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTOZj7HfFpyY"
   },
   "source": [
    "$$MSE\\left(X,h_{\\mathbf{w}}\\right)=\\dfrac{1}{m}\\sum_{i=1}^{m}\\left(\\mathbf{w}^T\\cdot\\mathbf{x}_i - y_i\\right )^2$$\n",
    "$$MSE\\left(X,h_{\\mathbf{w}}\\right)=\\dfrac{1}{m}\\Vert\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\Vert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example below is a plot of an MSE function where the true target value is 100, and the predicted values range between -10,000 to 10,000. The MSE loss (Y-axis) reaches its minimum value at prediction (X-axis) = 100. The range is 0 to ∞.\n",
    "![Plot of MSE Loss (Y-axis) vs. Predictions (X-axis)](MSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORU-9tCxFpyZ"
   },
   "source": [
    "- To find the value of $\\mathbf{w}$ that minimizes the MSE cost function, the most common way (*we have known since high school*) is to solve the derivative (gradient) equation. \n",
    "$$\\mathbf{\\hat{w}}=\\left(\\mathbf{X}^T  \\mathbf{X}\\right)^{\\dagger}  \\mathbf{X}^T  \\mathbf{y}$$\n",
    "  - $\\mathbf{\\hat{w}}$ is the value of $\\mathbf{w}$ that minimizes the cost function\n",
    "  - **Notice that** $\\mathbf{X}^T  \\mathbf{X}$ is not always invertible. $\\left(\\mathbf{X}^T  \\mathbf{X}\\right)^{\\dagger}$ is pseudo-inverse of $\\left(\\mathbf{X}^T \\mathbf{X}\\right)$ - a general case of inverse when the matrix is not invertible or not even square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Tgy-tRYFpyZ"
   },
   "source": [
    "### 3. Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qauCdk7LFpya"
   },
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70Mis-p9Fpyd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn.datasets as datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRr06hARFpyk"
   },
   "source": [
    "#### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0K3G_SOFpyk"
   },
   "outputs": [],
   "source": [
    "X,y=datasets.make_regression(n_samples=100,n_features=1, noise=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBFWzeY3Fpyp"
   },
   "source": [
    "#### Load and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BpxLtG3Fpyq"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize data \n",
    "\n",
    "plt.plot(X, y, 'ro')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLDAEVR8Fpyx"
   },
   "source": [
    "**TODO:** \n",
    "\n",
    "- Your observation about data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrb7peM1Fpyz"
   },
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdPXTgoAFpyz"
   },
   "outputs": [],
   "source": [
    "def train_linear_regression(X, y):\n",
    "    '''\n",
    "    Trains Linear Regression on the dataset (X, y).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (m, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    y : numpy array, shape (m, 1)\n",
    "        The vector of outputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : numpy array, shape (d + 1, 1)\n",
    "        The vector of parameters of Linear Regression after training.\n",
    "    '''\n",
    "    # TODO\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDgQ-5EDFpy5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_added_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12038/3106216188.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# First column of one_added_X is all ones (corresponding to x_0).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'one_added_X.shape ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_added_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'y.shape ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'one_added_X' is not defined"
     ]
    }
   ],
   "source": [
    "# Construct one_added_X \n",
    "# TODO:\n",
    "# First column of one_added_X is all ones (corresponding to x_0).\n",
    "\n",
    "print ('one_added_X.shape =', one_added_X.shape)\n",
    "print ('y.shape =', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVhd2dvCFpzE"
   },
   "source": [
    "#### Train our model and visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3YvmkEEFpzE"
   },
   "outputs": [],
   "source": [
    "w = train_linear_regression(one_added_X, y)\n",
    "\n",
    "# Visualize result\n",
    "predicted_ys = one_added_X.dot(w)\n",
    "\n",
    "plt.plot(X,y,'ro')\n",
    "\n",
    "x_min, x_max = plt.xlim()\n",
    "xs = np.array([x_min, x_max]).reshape(-1, 1)\n",
    "\n",
    "# Construct one_added_xs \n",
    "# TODO:\n",
    "# First column of one_added_xs is all ones (corresponding to x_0).\n",
    "\n",
    "\n",
    "predicted_ys = ones_added_xs.dot(w)\n",
    "plt.plot(xs, predicted_ys)\n",
    "plt.xlim(x_min, x_max)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTO6ilruFpzH"
   },
   "source": [
    "- **TODO**: Discuss about advantages and disadvantages of `Linear Regression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BehaTobaFpzI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab01-LinearRegression.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1992a7e580e2685c3441d0f3cc83cefc5ba6785739ef6e6382f107848fcf3f29"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
